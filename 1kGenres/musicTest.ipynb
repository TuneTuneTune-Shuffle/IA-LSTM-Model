{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51735173",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"  # Desactiva GPU\n",
    "os.environ[\"TF_XLA_FLAGS\"] = \"--tf_xla_enable_xla_devices=false\"  # Desactiva compilaci√≥n JIT\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pyaudio\n",
    "import wave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01ffdd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grabar_audio(nombre_archivo=\"grabacion.wav\", duracion=30, sample_rate=22050):\n",
    "    formato = pyaudio.paInt16\n",
    "    canales = 1\n",
    "    chunk = 1024\n",
    "    audio = pyaudio.PyAudio()\n",
    "    stream = audio.open(format=formato, channels=canales,\n",
    "                        rate=sample_rate, input=True,\n",
    "                        frames_per_buffer=chunk)\n",
    "\n",
    "    print(\"üéôÔ∏è Grabando durante\", duracion, \"segundos...\")\n",
    "    frames = []\n",
    "\n",
    "    for _ in range(0, int(sample_rate / chunk * duracion)):\n",
    "        data = stream.read(chunk)\n",
    "        frames.append(data)\n",
    "\n",
    "    print(\"‚úÖ Grabaci√≥n terminada.\")\n",
    "\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    audio.terminate()\n",
    "\n",
    "    with wave.open(nombre_archivo, 'wb') as wf:\n",
    "        wf.setnchannels(canales)\n",
    "        wf.setsampwidth(audio.get_sample_size(formato))\n",
    "        wf.setframerate(sample_rate)\n",
    "        wf.writeframes(b''.join(frames))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46b846fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def audio_a_mfcc(archivo_audio, max_pad_len=862):  # ajusta este valor a tu dataset\n",
    "    y, sr = librosa.load(archivo_audio, sr=22050)\n",
    "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40)\n",
    "\n",
    "    if mfcc.shape[1] < max_pad_len:\n",
    "        pad_width = max_pad_len - mfcc.shape[1]\n",
    "        mfcc = np.pad(mfcc, pad_width=((0, 0), (0, pad_width)), mode='constant')\n",
    "    else:\n",
    "        mfcc = mfcc[:, :max_pad_len]\n",
    "        \n",
    "    return mfcc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "082f7dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predecir_genero_desde_audio(audio_path, modelo, etiquetas=None):\n",
    "    # Paso 1: Cargar audio y extraer MFCC\n",
    "    audio_series, sample_rate = librosa.load(audio_path, duration=30.0, res_type=\"soxr_hq\")\n",
    "    mfcc = librosa.feature.mfcc(y=audio_series, sr=sample_rate, n_mfcc=40).T\n",
    "\n",
    "    # Paso 2: Padding o recorte\n",
    "    desired_length = 1300\n",
    "    if mfcc.shape[0] < desired_length:\n",
    "        pad_width = desired_length - mfcc.shape[0]\n",
    "        mfcc = np.pad(mfcc, ((0, pad_width), (0, 0)), mode='constant')\n",
    "    elif mfcc.shape[0] > desired_length:\n",
    "        mfcc = mfcc[:desired_length, :]\n",
    "\n",
    "    # Paso 3: Expandir dimensi√≥n para batch\n",
    "    entrada = mfcc[np.newaxis, ...]  # (1, 1300, 40)\n",
    "\n",
    "    # Paso 4: Predecir\n",
    "    prediccion = modelo.predict(entrada)\n",
    "    indice = np.argmax(prediccion)\n",
    "    confianza = prediccion[0][indice]\n",
    "    genero = etiquetas[indice] if etiquetas else indice\n",
    "    return genero, confianza\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13293037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéôÔ∏è Grabando durante 30 segundos...\n",
      "‚úÖ Grabaci√≥n terminada.\n",
      "Forma de entrada: [[-388.8929    -355.73294   -350.6865    ... -329.34967   -327.11124\n",
      "  -343.36945  ]\n",
      " [  64.098526    77.31606     79.23186   ...   50.90915     42.084923\n",
      "    46.05301  ]\n",
      " [ -52.434376   -55.724068   -57.874985  ...  -38.218025   -26.128994\n",
      "   -23.494293 ]\n",
      " ...\n",
      " [  -9.213314    -9.239636   -10.564787  ...   -6.000933    -7.1496296\n",
      "   -10.122371 ]\n",
      " [ -10.0705805   -7.6585      -9.575674  ...   -4.182612    -3.8900504\n",
      "    -5.983966 ]\n",
      " [   5.077876     1.2550437    1.5002912 ...   10.255703    13.931439\n",
      "    14.495608 ]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling Sequential.call().\n\n\u001b[1mInvalid input shape for input Tensor(\"data:0\", shape=(32, 862), dtype=float32). Expected shape (None, 1300, 40), but input has incompatible shape (32, 862)\u001b[0m\n\nArguments received by Sequential.call():\n  ‚Ä¢ inputs=tf.Tensor(shape=(32, 862), dtype=float32)\n  ‚Ä¢ training=False\n  ‚Ä¢ mask=None\n  ‚Ä¢ kwargs=<class 'inspect._empty'>",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     11\u001b[39m mfcc = audio_a_mfcc(\u001b[33m\"\u001b[39m\u001b[33mgrabacion.wav\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Predecir\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m genero, confianza = \u001b[43mpredecir_genero\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmfcc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodelo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43metiquetas\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33müéß G√©nero detectado: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgenero\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m (confianza: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfianza\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 6\u001b[39m, in \u001b[36mpredecir_genero\u001b[39m\u001b[34m(mfcc, modelo, etiquetas)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpredecir_genero\u001b[39m(mfcc, modelo, etiquetas=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m      2\u001b[39m     \u001b[38;5;66;03m# entrada = mfcc[np, ...]  # (1, n_frames, 40)\u001b[39;00m\n\u001b[32m      4\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mForma de entrada:\u001b[39m\u001b[33m\"\u001b[39m, mfcc)  \u001b[38;5;66;03m# Para depurar\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     prediccion = \u001b[43mmodelo\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmfcc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m     indice = np.argmax(prediccion)\n\u001b[32m      8\u001b[39m     confianza = prediccion[\u001b[32m0\u001b[39m][indice]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001b[32m    120\u001b[39m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[32m    121\u001b[39m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    124\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\models\\functional.py:276\u001b[39m, in \u001b[36mFunctional._adjust_input_rank\u001b[39m\u001b[34m(self, flat_inputs)\u001b[39m\n\u001b[32m    274\u001b[39m             adjusted.append(ops.expand_dims(x, axis=-\u001b[32m1\u001b[39m))\n\u001b[32m    275\u001b[39m             \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m276\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    277\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInvalid input shape for input \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. Expected shape \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    278\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mref_shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, but input has incompatible shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    279\u001b[39m     )\n\u001b[32m    280\u001b[39m \u001b[38;5;66;03m# Add back metadata.\u001b[39;00m\n\u001b[32m    281\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(flat_inputs)):\n",
      "\u001b[31mValueError\u001b[39m: Exception encountered when calling Sequential.call().\n\n\u001b[1mInvalid input shape for input Tensor(\"data:0\", shape=(32, 862), dtype=float32). Expected shape (None, 1300, 40), but input has incompatible shape (32, 862)\u001b[0m\n\nArguments received by Sequential.call():\n  ‚Ä¢ inputs=tf.Tensor(shape=(32, 862), dtype=float32)\n  ‚Ä¢ training=False\n  ‚Ä¢ mask=None\n  ‚Ä¢ kwargs=<class 'inspect._empty'>"
     ]
    }
   ],
   "source": [
    "# Cargar el modelo\n",
    "modelo = keras.models.load_model(\"genre_classifier_model.keras\")\n",
    "\n",
    "# Opcional: etiquetas si las tienes\n",
    "etiquetas = [\"blues\", \"classical\", \"country\", \"disco\", \"hiphop\", \"jazz\", \"metal\", \"pop\", \"reggae\", \"rock\"]  # por ejemplo\n",
    "\n",
    "# Grabar\n",
    "grabar_audio(\"grabacion.wav\", duracion=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "96736a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Procesar\n",
    "mfcc = audio_a_mfcc(\"grabacion.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dca0de40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Predecir\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m genero, confianza = \u001b[43mpredecir_genero_desde_audio\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgrabacion.wav\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodelo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43metiquetas\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33müéµ G√©nero detectado: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgenero\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m (confianza: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfianza\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 21\u001b[39m, in \u001b[36mpredecir_genero_desde_audio\u001b[39m\u001b[34m(audio_path, modelo, etiquetas)\u001b[39m\n\u001b[32m     19\u001b[39m indice = np.argmax(prediccion)\n\u001b[32m     20\u001b[39m confianza = prediccion[\u001b[32m0\u001b[39m][indice]\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m genero = \u001b[43metiquetas\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindice\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m etiquetas \u001b[38;5;28;01melse\u001b[39;00m indice\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m genero, confianza\n",
      "\u001b[31mIndexError\u001b[39m: list index out of range"
     ]
    }
   ],
   "source": [
    "# Predecir\n",
    "genero, confianza = predecir_genero_desde_audio(\"grabacion.wav\", modelo, etiquetas)\n",
    "print(f\"üéµ G√©nero detectado: {genero} (confianza: {confianza:.2f})\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
